import sys
import os
import json
import pandas as pd
import re
from datetime import datetime
import fitz  # PyMuPDF

# src ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from dotenv import load_dotenv
load_dotenv()

from src.tools import policy_tool
from src.tools.esg_policy_tool import _engine

# íŒŒì¼ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
REPORT_PATH = os.path.join(BASE_DIR, "data", "company", "ì‚¼ì„±ë¬¼ì‚°_ESGë³´ê³ ì„œ.pdf")
OUTPUT_PATH = "esg_analysis_report.md"
SUBCONTRACTOR_INFO = "ìœ¤ì£¼ê±´ì„¤ (ì² ê·¼ì½˜í¬ë¦¬íŠ¸ ë° ë¹„ê³„ ê³µì‚¬ ì „ë¬¸)"

def extract_text_from_pdf(pdf_path, specific_pages=None):
    """PDFì—ì„œ íŠ¹ì • í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (0-indexed)"""
    print(f"ğŸ“„ PDF ë¡œë”©: {pdf_path}")
    doc = fitz.open(pdf_path)
    text = ""
    
    # í˜ì´ì§€ ì§€ì •ì´ ì—†ìœ¼ë©´ ì• 5í˜ì´ì§€ë§Œ (ê¸°ì¡´ ë¡œì§)
    if not specific_pages:
        target_indices = range(5)
    else:
        target_indices = specific_pages

    print(f"   Reading pages: {target_indices}")
    for i in target_indices:
        if i < len(doc):
            page = doc[i]
            text += f"\n--- Page {i+1} ---\n"
            text += page.get_text()
            
    doc.close()
    return text

def find_kpi_target_pages(pdf_path, max_search_pages=30, top_k=3):
    """PDFì—ì„œ KPI/ëª©í‘œ í…Œì´ë¸”ì´ ìˆëŠ” í˜ì´ì§€ë“¤ì„ ìë™ìœ¼ë¡œ íƒìƒ‰ (ìƒìœ„ kê°œ)"""
    print(f"ğŸ” KPI ëª©í‘œ í˜ì´ì§€ ìë™ íƒìƒ‰ ì¤‘... (ìµœëŒ€ {max_search_pages}í˜ì´ì§€, ìƒìœ„ {top_k}ê°œ)")
    doc = fitz.open(pdf_path)
    
    keywords = {
        "KPI": 3, "ëª©í‘œ": 2, "ì‹¤ì ": 2, "tCO2e": 3, 
        "ì¬ìƒì—ë„ˆì§€": 2, "ì¤‘ëŒ€ì¬í•´": 2, "%": 1, "ë‹¬ì„±": 1
    }
    
    page_scores = []
    
    for i, page in enumerate(doc):
        if i >= max_search_pages: break
        
        text = page.get_text()
        score = 0
        for kw, points in keywords.items():
            score += text.count(kw) * points
            
        # í…Œì´ë¸” í—¤ë” ì¶”ì • ("ëª©í‘œ"ì™€ "ì‹¤ì "ì´ ê°™ì´ ë‚˜ì˜¤ë©´ ê°€ì‚°ì )
        if "ëª©í‘œ" in text and "ì‹¤ì " in text:
            score += 5
            
        # ì˜ë¯¸ ìˆëŠ” ì ìˆ˜ë§Œ ì €ì¥ (threshold = 10)
        if score > 10:
            page_scores.append((i, score))
            
    doc.close()
    
    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    page_scores.sort(key=lambda x: x[1], reverse=True)
    
    # ìƒìœ„ kê°œ ì¶”ì¶œ
    top_pages = [idx for idx, _ in page_scores[:top_k]]
    
    if top_pages:
        print(f"   âœ… Best KPI Pages Found: {[p+1 for p in top_pages]}")
        return top_pages
    else:
        print("   âš ï¸ KPI Page not found, defaulting to Page 13")
        return [12] # Default fallback

def generate_report():
    print("ğŸš€ ESG ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
    
    if not _engine:
        print("âŒ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return

    # 1. ì›ì²­ ë°ì´í„° ë¡œë“œ (ì‚¼ì„±ë¬¼ì‚° ë³´ê³ ì„œ)
    # ë¶„ì„ ê²°ê³¼: Page 15, 16 (ê·œì œ/íê¸°ë¬¼/ì¤‘ëŒ€ì¬í•´), 25 (ë²•ê·œ), 42 (í™˜ê²½), 59 (ì•ˆì „ë³´ê±´ë²•)
    # 0-based index: 14, 15, 24, 41, 58
    key_pages = [14, 15, 24, 41, 58]
    raw_text = extract_text_from_pdf(REPORT_PATH, specific_pages=key_pages)
    
    # 2. ë¦¬í¬íŠ¸ ì‘ì„±
    report_content = "# ğŸ—ï¸ í˜‘ë ¥ì‚¬ìš© ESG ê°€ì´ë“œë¼ì¸ ë¶„ì„ ë¦¬í¬íŠ¸\n"
    report_content += f"**ëŒ€ìƒ ì›ì²­**: ì‚¼ì„±ë¬¼ì‚° (ì¶œì²˜: {os.path.basename(REPORT_PATH)})\n"
    report_content += f"**ìˆ˜ì‹ **: {SUBCONTRACTOR_INFO}\n\n"
    
    # [Section 1] ì§€ì¹¨ í•´ì„¤
    print("   ğŸ” [1/3] ì§€ì¹¨ í•´ì„¤ ìƒì„± ì¤‘...")
    explanation = _engine.explain_guideline(raw_text[:30000]) # í…ìŠ¤íŠ¸ ê¸¸ì´ ìƒí–¥ (3ë§Œì)
    report_content += "## 1. ì›ì²­ ESG ì§€ì¹¨ í•´ì„¤\n"
    report_content += explanation + "\n\n"
    
    # [Section 2] ì²´í¬ë¦¬ìŠ¤íŠ¸
    print("   ğŸ“ [2/3] ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„± ì¤‘ (Excelìš© ëŒ€ëŸ‰ ìƒì„±)...")
    checklist_json_str = _engine.generate_checklist(raw_text[:30000], SUBCONTRACTOR_INFO)
    
    # JSON Parsing & Excel Export
    try:
        # Markdown Code Block ì œê±° (```json ... ```)
        cleaned_json = re.sub(r"```json\s*|\s*```", "", checklist_json_str, flags=re.DOTALL).strip()
        checklist_data = json.loads(cleaned_json)
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(checklist_data)
        
        # [User Request] 'importance' ì»¬ëŸ¼ ì œê±°
        if 'importance' in df.columns:
            df = df.drop(columns=['importance'])
            
        # [User Request] 'item' ë˜ëŠ” 'question' ì»¬ëŸ¼ì—ì„œ '(Yes/No)' í…ìŠ¤íŠ¸ ì œê±°
        for col in ['item', 'question', 'ì ê²€í•­ëª©']:
            if col in df.columns:
                # ì •ê·œì‹ìœ¼ë¡œ (Yes/No), (yes/no) ë“± ì œê±°í•˜ê³  ê³µë°± ì •ë¦¬
                df[col] = df[col].astype(str).str.replace(r"\(Yes/No\)", "", case=False, regex=True).str.strip()
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        excel_filename = f"esg_checklist_{timestamp}.xlsx"
        df.to_excel(excel_filename, index=False)
        print(f"   ğŸ’¾ Excel ì €ì¥ ì™„ë£Œ: {excel_filename} ({len(df)} items)")
        
        report_content += f"## 2. í˜„ì¥ ì‹¤ë¬´ììš© ì²´í¬ë¦¬ìŠ¤íŠ¸ ({SUBCONTRACTOR_INFO})\n"
        report_content += f"**âœ… ìƒì„¸ ì²´í¬ë¦¬ìŠ¤íŠ¸ëŠ” ë³„ë„ ì—‘ì…€ íŒŒì¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: `{excel_filename}`**\n\n"
        report_content += f"ì´ {len(df)}ê°œì˜ ì ê²€ í•­ëª©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ì•ˆì „, í™˜ê²½, ì¸ê¶Œ ë“±)\n\n"
        
    except Exception as e:
        print(f"   âš ï¸ Excel ìƒì„± ì‹¤íŒ¨ (JSON íŒŒì‹± ì˜¤ë¥˜ ë“±): {e}")
        report_content += f"## 2. í˜„ì¥ ì‹¤ë¬´ììš© ì²´í¬ë¦¬ìŠ¤íŠ¸\n(Excel ìƒì„± ì‹¤íŒ¨ë¡œ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤)\n{checklist_json_str}\n\n"

    
    # [Section 3] Gap Analysis (Target from Page 13)
    print("   ğŸ“Š [3/3] ëª©í‘œ vs ì‹¤ì  ë¹„êµ ë¶„ì„ ì¤‘...")
    
    # Page 13 (Index 12) - KPI ëª©í‘œ í…Œì´ë¸” í˜ì´ì§€ ì¶”ì¶œ
    # [Dynamic] ìë™ìœ¼ë¡œ KPI í˜ì´ì§€ íƒìƒ‰ (ë‹¤ì¤‘ í˜ì´ì§€)
    target_page_indices = find_kpi_target_pages(REPORT_PATH)
    target_page_text = extract_text_from_pdf(REPORT_PATH, specific_pages=target_page_indices)
    
    # í•˜ì²­ ì‹¤ì  (ê°€ìƒ ë°ì´í„° - ì‚¼ì„±ë¬¼ì‚° ëª©í‘œ í•­ëª©ì— ë§ì¶° êµ¬ì²´í™”)
    actual_data = """
    - ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œëŸ‰: 1,200 tCO2e (ì „ë…„ ëŒ€ë¹„ 3% ê°ì†Œ)
    - ì¬ìƒì—ë„ˆì§€ ì‚¬ìš©ë¥ : 15% (íƒœì–‘ê´‘ íŒ¨ë„ ì¼ë¶€ ì„¤ì¹˜)
    - ì¤‘ëŒ€ì¬í•´: 3ê±´
    - íê¸°ë¬¼ ì¬í™œìš©ë¥ : 85%
    - ì•ˆì „êµìœ¡ ì´ìˆ˜ìœ¨: 95%
    """
    
    gap_analysis = _engine.analyze_gap(target_page_text, actual_data)
    report_content += "## 3. ëª©í‘œ ëŒ€ë¹„ ì„±ê³¼ ë¶„ì„ (Gap Analysis)\n"
    report_content += f"> **ë¹„êµ ê¸°ì¤€**: ì‚¼ì„±ë¬¼ì‚° 2025 ì§€ì†ê°€ëŠ¥ê²½ì˜ë³´ê³ ì„œ ë‚´ 'KPI ì´í–‰í˜„í™© ë° ëª©í‘œ' (ìë™ íƒì§€ëœ Pages {[p+1 for p in target_page_indices]})\n\n"
    report_content += gap_analysis + "\n\n"
    
    # íŒŒì¼ ì €ì¥
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(report_content)
        
    print(f"âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {OUTPUT_PATH}")

if __name__ == "__main__":
    generate_report()