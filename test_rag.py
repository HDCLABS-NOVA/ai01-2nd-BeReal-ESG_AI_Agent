
import os
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Load environment variables (API KEY)
load_dotenv()

VECTOR_DB_DIR = "vector_db/esg_all"

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def test_rag_generation():
    if not os.path.exists(VECTOR_DB_DIR):
        print(f"âŒ Directory not found: {VECTOR_DB_DIR}")
        return

    print("ğŸ”Œ Loading Vector DB & LLM...")
    
    # 1. Setup Retrieval
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    vector_db = Chroma(
        collection_name="esg_all",
        embedding_function=embeddings,
        persist_directory=VECTOR_DB_DIR
    )
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})

    # 2. Setup LLM & Prompt
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    template = """
    ë‹¹ì‹ ì€ ESG ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [Context]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    
    [Context]
    {context}
    
    Question: {question}
    
    Answer (in Korean):
    """
    prompt = ChatPromptTemplate.from_template(template)

    # 3. Build Chain
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    # 4. Run Test
    query = "ìµœê·¼ ê·œì œ ë™í–¥ì€?"
    print(f"\nâ“ ì§ˆë¬¸: '{query}'")
    print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘ (LLM í˜¸ì¶œ)...")
    
    try:
        response = rag_chain.invoke(query)
        print("\n" + "="*50)
        print(response)
        print("="*50 + "\n")
        
    except Exception as e:
        print(f"âŒ Error: {e}")

if __name__ == "__main__":
    test_rag_generation()
