import os
import time
import json
import schedule
import requests
import numpy as np
import fitz  # PyMuPDF
from datetime import datetime
from typing import List, Dict, Optional
from dotenv import load_dotenv

# Selenium (ë¸Œë¼ìš°ì € ì œì–´ìš©)
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# LangChain & AI
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sklearn.metrics.pairwise import cosine_similarity

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì „ì—­ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(BASE_DIR, "data")
DOWNLOAD_DIR = os.path.join(DATA_DIR, "domestic")
HISTORY_DIR = os.path.join(DATA_DIR, "crawling")
HISTORY_FILE = os.path.join(HISTORY_DIR, "crawl_history.json")
VECTOR_DB_DIR = os.path.join(DATA_DIR, "chroma_db")  # ë²¡í„°DB ì €ì¥ ê²½ë¡œ

# [ë³€ê²½] ëª¨ë‹ˆí„°ë§ íƒ€ê²Ÿ ëª©ë¡
# law.go.krì€ ë³„ë„ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ typeì„ êµ¬ë¶„í•˜ê±°ë‚˜ URLë¡œ ì‹ë³„
MINISTRY_TARGETS = [
    {
        "name": "í™˜ê²½ë¶€(êµ­ê°€ë²•ë ¹ì„¼í„°)",
        "url": "https://www.law.go.kr/nwRvsLsPop.do?cptOfi=1482000",
        "type": "LAW_GO_KR",  # ì „ìš© íƒ€ì… ì§€ì •
        "page_param": None
    },
    {
        "name": "ê³ ìš©ë…¸ë™ë¶€(MOEL)",
        "url": "https://www.moel.go.kr/info/lawinfo/lawmaking/list.do", 
        "type": "GENERIC_BOARD",
        "page_param": "pageIndex"
    },
    {
        "name": "êµ­í† êµí†µë¶€(MOLIT)",
        "url": "http://www.molit.go.kr/USR/LEGAL/m_35/lst.jsp",        
        "type": "GENERIC_BOARD",
        "page_param": "page"
    }
]

class RegulationMonitor:
    """
    [ê·œì œ ëª¨ë‹ˆí„°ë§ ì—”ì§„ - AI Enhanced]
    1. Seleniumìœ¼ë¡œ ë³´ê³ ì„œ ë° ë²•ë ¹ì•ˆ ìë™ ë‹¤ìš´ë¡œë“œ (ê¸ˆìœµìœ„/GMI + í™˜ê²½/êµ­í† /ë…¸ë™ë¶€)
    2. êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°(law.go.kr) ì „ìš© í¬ë¡¤ëŸ¬ íƒ‘ì¬ (í…ìŠ¤íŠ¸ ì¶”ì¶œ -> íŒŒì¼ ì €ì¥)
    3. GPT-4oë¥¼ ì´ìš©í•´ ë¬¸ì„œì˜ ì¤‘ìš”ë„ í‰ê°€ ë° ì„ ë³„ (Filtering)
    4. ì„ ë³„ëœ ì¤‘ìš” ë¬¸ì„œë§Œ Vector DBì— ìë™ ì €ì¥ (RAG ì¤€ë¹„)
    """
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RegulationMonitor, cls).__new__(cls)
            cls._instance._initialize()
        return cls._instance

    def _initialize(self):
        print("âš™ï¸ [RegulationMonitor] ì´ˆê¸°í™” ì¤‘...")
        
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-m3",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.embeddings = None

        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

        if self.embeddings:
            self.vector_db = Chroma(
                collection_name="esg_regulations",
                embedding_function=self.embeddings,
                persist_directory=VECTOR_DB_DIR
            )
        else:
            self.vector_db = None

        self.tavily = TavilySearchResults(max_results=5)
        
        os.makedirs(DOWNLOAD_DIR, exist_ok=True)
        os.makedirs(HISTORY_DIR, exist_ok=True)
        os.makedirs(VECTOR_DB_DIR, exist_ok=True)
        
        self.history = self._load_history()

    def _load_history(self) -> Dict:
        if os.path.exists(HISTORY_FILE):
            try:
                with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}

    def _save_history(self):
        try:
            with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _is_processed(self, url: str) -> bool:
        return url in self.history

    def _mark_as_processed(self, url: str, title: str, files: List[str]):
        self.history[url] = {
            "title": title,
            "processed_at": datetime.now().isoformat(),
            "files": files
        }
        self._save_history()

    def _extract_text_preview(self, file_path: str, max_pages: int = 3) -> str:
        """íŒŒì¼ ë‚´ìš© í”„ë¦¬ë·° ì¶”ì¶œ (PDF ë° TXT ì§€ì›)"""
        text_preview = ""
        try:
            if file_path.lower().endswith('.pdf'):
                doc = fitz.open(file_path)
                for i, page in enumerate(doc):
                    if i >= max_pages: break
                    text_preview += page.get_text()
                doc.close()
            elif file_path.lower().endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    text_preview = f.read(3000) # ì•ë¶€ë¶„ 3000ì
            else:
                text_preview = "(ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤)"
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({os.path.basename(file_path)}): {e}")
        return text_preview

    def _analyze_and_store(self, file_path: str, title: str, source: str) -> bool:
        if not self.vector_db:
            return False

        filename = os.path.basename(file_path)
        print(f"   ğŸ§  [AI ë¶„ì„] '{filename}' ì¤‘ìš”ë„ í‰ê°€ ì¤‘...")

        content_preview = self._extract_text_preview(file_path)
        if not content_preview:
            return False

        prompt = f"""
        ë‹¹ì‹ ì€ ESG ë° ì‚°ì—… ì•ˆì „, í™˜ê²½ ê·œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì¶œì²˜: '{source}'
        ë¬¸ì„œ ì œëª©: '{title}'
        ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:
        {content_preview[:2000]}

        ì´ ë¬¸ì„œê°€ ê¸°ì—…ì˜ ESG ê²½ì˜, í™˜ê²½ ê·œì œ ì¤€ìˆ˜, ì‚°ì—… ì•ˆì „, í˜¹ì€ ì»´í”Œë¼ì´ì–¸ìŠ¤ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” **ì¤‘ìš”í•œ ë²•ë ¹/ê°€ì´ë“œë¼ì¸**ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
        
        [íŒë‹¨ ê¸°ì¤€]
        - ì¤‘ìš”(High): ë²•ë¥ /ì‹œí–‰ë ¹ ê°œì •ì•ˆ, ì…ë²•ì˜ˆê³ , ì²˜ë²Œ ê¸°ì¤€ ê°•í™”, ê³µì‹œ ì˜ë¬´í™” ê°€ì´ë“œë¼ì¸
        - ë³´í†µ(Medium): ë‹¨ìˆœ ì‹¤íƒœì¡°ì‚¬ ê²°ê³¼, ìº í˜ì¸ì„± ì•Œë¦¼, ì¸ì‚¬ ë°œë ¹
        - ë‚®ìŒ(Low): í–‰ì‚¬/ì„¸ë¯¸ë‚˜ ì•Œë¦¼, ë‹¨ìˆœ í™ë³´ë¬¼

        ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
        {{
            "is_important": true/false,
            "score": (1~10),
            "reason": "í•œ ì¤„ ìš”ì•½",
            "category": "ë²•ë ¹ê°œì •/ê°€ì´ë“œë¼ì¸/ë‹¨ìˆœì•Œë¦¼"
        }}
        """
        
        try:
            response = self.llm.invoke(prompt)
            response_text = response.content.replace("```json", "").replace("```", "").strip()
            analysis = json.loads(response_text)
            
            is_important = analysis.get("is_important", False)
            score = analysis.get("score", 0)
            
            print(f"      ğŸ‘‰ ê²°ê³¼: ì¤‘ìš”ë„ {score}ì  ({analysis.get('reason')})")

            if is_important and score >= 6:
                print(f"      ğŸ’¾ [Vector DB] ì¤‘ìš” ë¬¸ì„œë¡œ ì‹ë³„ë˜ì–´ DBì— ì €ì¥í•©ë‹ˆë‹¤.")
                
                full_text = ""
                # PDF ì²˜ë¦¬
                if file_path.lower().endswith('.pdf'):
                    full_doc = fitz.open(file_path)
                    for page in full_doc:
                        full_text += page.get_text()
                    full_doc.close()
                # TXT ì²˜ë¦¬ (law.go.kr ë“±)
                elif file_path.lower().endswith('.txt'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        full_text = f.read()
                
                if full_text:
                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                    chunks = text_splitter.create_documents(
                        [full_text], 
                        metadatas=[{
                            "source": source,
                            "title": title,
                            "filename": filename,
                            "category": analysis.get("category", "Uncategorized"),
                            "crawled_at": datetime.now().isoformat()
                        }]
                    )
                    self.vector_db.add_documents(chunks)
                    print(f"      âœ… DB ì €ì¥ ì™„ë£Œ ({len(chunks)} chunks)")
                return True
            else:
                print(f"      ğŸ—‘ï¸ [Discard] ì¤‘ìš”ë„ê°€ ë‚®ì•„ DBì— ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False

        except Exception as e:
            print(f"      âŒ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _get_chrome_driver(self):
        chrome_options = Options()
        chrome_options.add_argument("--headless=new") 
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--ignore-certificate-errors")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        prefs = {
            "download.default_directory": DOWNLOAD_DIR,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True,
            "plugins.always_open_pdf_externally": True,
            "profile.default_content_settings.popups": 0
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        service = ChromeService(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        return driver

    def _fetch_law_go_kr(self, driver, target_info: Dict) -> List[Dict]:
        """
        [ì „ìš©] êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°(law.go.kr) í¬ë¡¤ëŸ¬
        - êµ¬ì¡°: ë¦¬ìŠ¤íŠ¸ -> í´ë¦­ -> ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë·°ì–´ (ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ ê¹Œë‹¤ë¡œì›€)
        - ì „ëµ: ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ .txt íŒŒì¼ë¡œ ì €ì¥
        """
        url = target_info["url"]
        source_name = target_info["name"]
        results = []

        print(f"ğŸ“¡ [{source_name}] ì ‘ì† ì¤‘... ({url})")
        try:
            driver.get(url)
            wait = WebDriverWait(driver, 15)
            # law.go.kr ë¦¬ìŠ¤íŠ¸ í…Œì´ë¸” ëŒ€ê¸° (tbody)
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "tbody")))
            
            # ìƒìœ„ 3ê°œ í•­ëª©
            for i in range(3):
                try:
                    row_index = i + 1
                    # ì œëª© ë§í¬ ì°¾ê¸° (ë³´í†µ 2ë²ˆì§¸ tdì˜ a íƒœê·¸, í˜¹ì€ text align left)
                    # law.go.krì€ êµ¬ì¡°ê°€ ê°€ë³€ì ì´ë¼ tr ë‚´ë¶€ì˜ 'a' íƒœê·¸ ì¤‘ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²ƒì„ ì°¾ìŒ
                    row = wait.until(EC.presence_of_element_located(
                        (By.CSS_SELECTOR, f"tbody tr:nth-child({row_index})")
                    ))
                    links = row.find_elements(By.TAG_NAME, "a")
                    
                    target_link = None
                    title = ""
                    for link in links:
                        text = link.text.strip()
                        if text and len(text) > 5: # ì œëª©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë§í¬
                            target_link = link
                            title = text
                            break
                    
                    if not target_link: continue

                    unique_key = f"{source_name}_{title}"
                    if self._is_processed(unique_key):
                        print(f"   â­ï¸ [Skip] {source_name}: {title}")
                        continue

                    print(f"   ğŸ” [New] {source_name} ë¶„ì„: {title}")
                    
                    # ìƒì„¸ í˜ì´ì§€ ì§„ì… (law.go.krì€ í´ë¦­ ì‹œ í˜ì´ì§€ ì´ë™/AJAX ë¡œë”©)
                    driver.execute_script("arguments[0].click();", target_link)
                    time.sleep(3) # ë¡œë”© ëŒ€ê¸°
                    
                    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (ë²•ë ¹ ë³¸ë¬¸ ì˜ì—­)
                    # law.go.kr ë³¸ë¬¸ ID í›„ë³´: contentBody, conScroll, viewArea ë“±
                    content_text = ""
                    try:
                        # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                        body_elem = None
                        for selector in ["#contentBody", ".lawCon", "#conScroll", "body"]:
                            try:
                                body_elem = driver.find_element(By.CSS_SELECTOR, selector)
                                if len(body_elem.text) > 100:
                                    break
                            except: continue
                        
                        if body_elem:
                            content_text = body_elem.text
                    except Exception as e:
                        print(f"      âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

                    downloaded_files = []
                    if content_text:
                        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
                        safe_title = "".join([c for c in title if c.isalnum() or c in (' ', '-', '_')]).rstrip()
                        file_name = f"{safe_title}.txt"
                        file_path = os.path.join(DOWNLOAD_DIR, file_name)
                        
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(f"ì œëª©: {title}\nì¶œì²˜: {url}\n\n{content_text}")
                        
                        print(f"      âœ… ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {file_name}")
                        downloaded_files.append(file_path)
                        
                        # AI ë¶„ì„ ë° ì €ì¥
                        self._analyze_and_store(file_path, title, source_name)

                    self._mark_as_processed(unique_key, title, downloaded_files)
                    results.append({"source": source_name, "title": title, "files": downloaded_files})
                    
                    # ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°€ê¸° (ë’¤ë¡œê°€ê¸° í˜¹ì€ URL ì¬ì ‘ì†)
                    driver.get(url)
                    wait.until(EC.presence_of_element_located((By.TAG_NAME, "tbody")))
                    
                except Exception as e:
                    print(f"      âš ï¸ ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    driver.get(url)
                    time.sleep(2)

        except Exception as e:
            print(f"âŒ [{source_name}] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            
        return results

    def _scrape_generic_board(self, driver, target_info: Dict) -> List[Dict]:
        """[ê³µí†µ] ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§"""
        base_url = target_info["url"]
        source_name = target_info["name"]
        page_param = target_info.get("page_param")
        results = []

        max_pages = 3 if page_param else 1
        
        for page in range(1, max_pages + 1):
            if page_param:
                sep = "&" if "?" in base_url else "?"
                target_url = f"{base_url}{sep}{page_param}={page}"
            else:
                target_url = base_url

            print(f"ğŸ“¡ [{source_name}] ì ‘ì† ì¤‘ (Page {page})...")
            try:
                driver.get(target_url)
                wait = WebDriverWait(driver, 15)
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
                
                for i in range(3):
                    try:
                        row_index = i + 1
                        # ì¼ë°˜ì ì¸ ê²Œì‹œíŒ: në²ˆì§¸ í–‰ì˜ ì œëª© ë§í¬ ì°¾ê¸°
                        # êµ¬ì¡°ê°€ ë‹¤ì–‘í•˜ë¯€ë¡œ, í–‰ ë‚´ë¶€ì—ì„œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ aíƒœê·¸ë¥¼ ì œëª©ìœ¼ë¡œ ì¶”ì •
                        row = wait.until(EC.presence_of_element_located(
                            (By.CSS_SELECTOR, f"table tbody tr:nth-child({row_index})")
                        ))
                        links = row.find_elements(By.TAG_NAME, "a")
                        
                        post_link = None
                        title = ""
                        for link in links:
                            text = link.text.strip()
                            if len(text) > 5: # ì œëª©ì¼ ê°€ëŠ¥ì„±
                                post_link = link
                                title = text
                                break
                        
                        if not post_link: continue
                        
                        unique_key = f"{source_name}_{title}"
                        
                        if self._is_processed(unique_key):
                            print(f"   â­ï¸ [Skip] {source_name}: {title}")
                            continue
                            
                        print(f"   ğŸ” [New] {source_name} ë¶„ì„: {title}")
                        
                        driver.execute_script("arguments[0].click();", post_link)
                        time.sleep(2)
                        
                        downloaded_files = []
                        potential_links = driver.find_elements(By.TAG_NAME, "a")
                        file_links = []
                        for link in potential_links:
                            href = link.get_attribute("href")
                            text = link.text.strip()
                            if href and ("down" in href.lower() or "file" in href.lower() or "download" in href.lower()) and any(ext in text.lower() for ext in ['.pdf', '.hwp', '.doc']):
                                file_links.append(link)
                        
                        for link in file_links[:1]:
                            f_name = link.text.strip()
                            print(f"      ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œë„: {f_name}")
                            before_files = set(os.listdir(DOWNLOAD_DIR))
                            driver.execute_script("arguments[0].click();", link)
                            
                            for _ in range(10):
                                time.sleep(1)
                                new_files = set(os.listdir(DOWNLOAD_DIR)) - before_files
                                if new_files:
                                    new_file = list(new_files)[0]
                                    if not new_file.endswith('.crdownload'):
                                        full_path = os.path.join(DOWNLOAD_DIR, new_file)
                                        downloaded_files.append(full_path)
                                        print(f"      âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {new_file}")
                                        self._analyze_and_store(full_path, title, source_name)
                                        break
                        
                        self._mark_as_processed(unique_key, title, downloaded_files)
                        results.append({"source": source_name, "title": title, "files": downloaded_files})
                        
                        driver.back()
                        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
                        time.sleep(1)
                        
                    except Exception as e:
                        print(f"      âš ï¸ ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘ ìŠ¤í‚µ: {e}")
                        if target_url not in driver.current_url:
                            driver.back()
                            time.sleep(1)

            except Exception as e:
                print(f"âŒ [{source_name}] Page {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
        return results

    def _fetch_gmi_reports_selenium(self) -> List[Dict]:
        target_url = "https://www.gmi.go.kr/np/boardList.do?menuCd=2090&seCd=2"
        results = []
        
        print(f"ğŸ“¡ [GMI] ì ‘ì† ë° ìŠ¤ìº” ì‹œì‘ ({target_url})")
        driver = self._get_chrome_driver()
        
        try:
            driver.get(target_url)
            wait = WebDriverWait(driver, 20)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
            
            for i in range(3):
                try:
                    row_index = i + 1
                    post_link = wait.until(EC.element_to_be_clickable(
                        (By.CSS_SELECTOR, f"table tbody tr:nth-child({row_index}) a")
                    ))
                    
                    title = post_link.text.strip() or driver.execute_script("return arguments[0].innerText;", post_link).strip()
                    unique_key = f"GMI_{title}"
                    
                    if self._is_processed(unique_key):
                        print(f"   â­ï¸ [Skip] ì´ë¯¸ ìˆ˜ì§‘ëœ ë³´ê³ ì„œ: {title}")
                        continue
                        
                    print(f"   ğŸ” [New] ì‹ ê·œ ë³´ê³ ì„œ ë¶„ì„: {title}")
                    driver.execute_script("arguments[0].click();", post_link)
                    time.sleep(2)
                    
                    downloaded_files = []
                    file_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='downloadAttach']")
                    if not file_links:
                        file_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='FileDown']")

                    for link in file_links:
                        f_name = link.text.strip() or driver.execute_script("return arguments[0].innerText;", link).strip()
                        if 'pdf' in f_name.lower():
                            print(f"      ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œë„: {f_name}")
                            before_files = set(os.listdir(DOWNLOAD_DIR))
                            driver.execute_script("arguments[0].click();", link)
                            for _ in range(15):
                                time.sleep(1)
                                new_files = set(os.listdir(DOWNLOAD_DIR)) - before_files
                                if new_files:
                                    downloaded_file = list(new_files)[0]
                                    if not downloaded_file.endswith('.crdownload'):
                                        full_path = os.path.join(DOWNLOAD_DIR, downloaded_file)
                                        downloaded_files.append(full_path)
                                        print(f"      âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded_file}")
                                        self._analyze_and_store(full_path, title, "GMI")
                                        break
                    
                    self._mark_as_processed(unique_key, title, downloaded_files)
                    results.append({"source": "GMI", "title": title, "files": downloaded_files})
                    driver.back()
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
                    time.sleep(1)
                except Exception as e:
                    print(f"      âš ï¸ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    if "boardList.do" not in driver.current_url:
                        driver.back()
                        time.sleep(2)
        except Exception as e:
            print(f"âŒ [GMI] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        finally:
            driver.quit()
        return results

    def _fetch_fsc_reports_selenium(self) -> List[Dict]:
        base_url = "https://www.fsc.go.kr/no010101"
        results = []
        
        print(f"ğŸ“¡ [FSC] ì ‘ì† ë° ìŠ¤ìº” ì‹œì‘ (1~3 í˜ì´ì§€ í™•ì¸)")
        driver = self._get_chrome_driver()
        
        try:
            for page in range(1, 4):
                target_url = f"{base_url}?curPage={page}"
                print(f"   ğŸ“„ FSC Page {page} ìŠ¤ìº” ì¤‘...")
                
                driver.get(target_url)
                wait = WebDriverWait(driver, 20)
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".board-list .subject a")))
                
                list_items = driver.find_elements(By.CSS_SELECTOR, ".board-list .subject a")
                keywords = ["ESG", "ê³µì‹œ", "ì§€ì†ê°€ëŠ¥", "ë…¹ìƒ‰", "ê¸°í›„", "íƒì†Œë…¸ë¯¸"]
                
                target_items = []
                for item in list_items:
                    text = item.text.strip()
                    if any(k in text for k in keywords):
                        href = item.get_attribute("href")
                        target_items.append((text, href))
                
                for title, link in target_items:
                    if self._is_processed(link):
                        print(f"      â­ï¸ [Skip] {title}")
                        continue
                    
                    print(f"      ğŸ” [New] ë¶„ì„: {title}")
                    driver.get(link)
                    time.sleep(2)
                    
                    downloaded_files = []
                    file_links = driver.find_elements(By.CSS_SELECTOR, ".file-list a")
                    
                    for f_link in file_links:
                        f_name = f_link.text.strip()
                        if any(ext in f_name.lower() for ext in ['.pdf', '.hwp']):
                            print(f"         ğŸ“¥ ë‹¤ìš´ë¡œë“œ í´ë¦­: {f_name}")
                            before_files = set(os.listdir(DOWNLOAD_DIR))
                            f_link.click()
                            for _ in range(15):
                                time.sleep(1)
                                new_files = set(os.listdir(DOWNLOAD_DIR)) - before_files
                                if new_files:
                                    new_file = list(new_files)[0]
                                    if not new_file.endswith('.crdownload'):
                                        full_path = os.path.join(DOWNLOAD_DIR, new_file)
                                        downloaded_files.append(full_path)
                                        if new_file.lower().endswith('.pdf'):
                                            self._analyze_and_store(full_path, title, "FSC")
                                        break
                    
                    self._mark_as_processed(link, title, downloaded_files)
                    results.append({"source": "FSC", "title": title, "files": downloaded_files})
                    
                    driver.get(target_url)
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".board-list .subject a")))
                    
        except Exception as e:
            print(f"âŒ [FSC] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        finally:
            driver.quit()
            
        return results

    def _fetch_legal_updates(self) -> List[Dict]:
        results = []
        driver = self._get_chrome_driver()
        try:
            for target in MINISTRY_TARGETS:
                try:
                    # [ë³€ê²½] ì‚¬ì´íŠ¸ íƒ€ì…ì— ë”°ë¼ ì „ìš© í¬ë¡¤ëŸ¬ ì‚¬ìš©
                    if target.get("type") == "LAW_GO_KR":
                        site_results = self._fetch_law_go_kr(driver, target)
                    else:
                        site_results = self._scrape_generic_board(driver, target)
                    results.extend(site_results)
                except Exception as e:
                    print(f"âŒ {target['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            driver.quit()
        return results

    def _deduplicate_news(self, news_list: List[Dict], threshold=0.85) -> List[Dict]:
        if not news_list or not self.embeddings: return news_list
        texts = [item['content'] for item in news_list]
        vectors = self.embeddings.embed_documents(texts)
        matrix = np.array(vectors)
        processed = [False] * len(news_list)
        unique_news = []
        for i in range(len(news_list)):
            if processed[i]: continue
            current_cluster = [news_list[i]]
            processed[i] = True
            vec_i = matrix[i].reshape(1, -1)
            if i + 1 < len(news_list):
                remaining_vectors = matrix[i+1:]
                similarities = cosine_similarity(vec_i, remaining_vectors)[0]
                for idx, score in enumerate(similarities):
                    real_idx = i + 1 + idx
                    if not processed[real_idx] and score >= threshold:
                        current_cluster.append(news_list[real_idx])
                        processed[real_idx] = True
            representative = max(current_cluster, key=lambda x: len(x['content']))
            representative['related_count'] = len(current_cluster) - 1
            unique_news.append(representative)
        return unique_news

    def monitor_all(self, query: str = "ESG ê·œì œ ë™í–¥") -> str:
        print("\n" + "="*50)
        print(f"ğŸ”„ [ëª¨ë‹ˆí„°ë§ ì‹¤í–‰] {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*50)

        # 1. ë³´ê³ ì„œ ìˆ˜ì§‘ (GMI, FSC)
        gmi_reports = self._fetch_gmi_reports_selenium()
        fsc_reports = self._fetch_fsc_reports_selenium()
        
        # 2. ë²•ë ¹ ì—…ë°ì´íŠ¸ ìˆ˜ì§‘
        legal_updates = self._fetch_legal_updates()
        
        reports = gmi_reports + fsc_reports + legal_updates
        
        # 3. ë‰´ìŠ¤ ê²€ìƒ‰
        news_results = []
        if os.getenv("TAVILY_API_KEY"):
            queries = list(set([query, "ESG ê³µì‹œ ì˜ë¬´í™”", "í™˜ê²½ë¶€ ì…ë²•ì˜ˆê³ ", "ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²• ê°œì •"]))
            for q in queries:
                try:
                    raw = self.tavily.invoke(q)
                    for item in raw:
                        news_results.append({
                            "title": item['content'][:30] + "...", 
                            "content": item['content'],
                            "url": item['url'],
                            "source": "Web News"
                        })
                except Exception as e:
                    print(f"âš ï¸ Tavily ê²€ìƒ‰ ì‹¤íŒ¨ ({q}): {e}")
        
        clean_news = self._deduplicate_news(news_results)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        result_str = f"## ğŸŒ ESG ê·œì œ & ë²•ë ¹ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ({time.strftime('%Y-%m-%d')})\n\n"
        
        if reports:
            result_str += "### ğŸ†• ì‹ ê·œ ë³´ê³ ì„œ ë° ë²•ë ¹ ê°œì •ì•ˆ\n"
            for r in reports:
                files_msg = ", ".join([os.path.basename(f) for f in r['files']]) if r['files'] else "íŒŒì¼ ì—†ìŒ"
                result_str += f"- **[{r['source']}]** {r['title']}\n"
                result_str += f"  - ğŸ’¾ ë‹¤ìš´ë¡œë“œ: `{files_msg}`\n"
        else:
            result_str += "### âœ… ì‹ ê·œ ìë£Œ ì—†ìŒ (ëª¨ë‘ ìµœì‹  ìƒíƒœ)\n"
            
        result_str += "\n### ğŸ“° ì£¼ìš” ë‰´ìŠ¤ ë° ì…ë²• ë™í–¥\n"
        if clean_news:
            for n in clean_news:
                result_str += f"- {n['content'][:100]}...\n  ğŸ”— [ê¸°ì‚¬]({n['url']})\n"
        
        print(result_str)
        return result_str

# LangChain Tool Export
_monitor_instance = RegulationMonitor()

@tool
def fetch_regulation_updates(query: str = "ESG regulatory updates") -> str:
    """
    Monitors ESG updates using Selenium and History Tracking to detect NEW reports only.
    Use GPT to filter important documents and store them in Vector DB.
    """
    return _monitor_instance.monitor_all(query)

def run_continuously(interval_days: int = 1):
    print(f"\nâ° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘: {interval_days}ì¼ë§ˆë‹¤ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    _monitor_instance.monitor_all()
    schedule.every(interval_days).days.do(_monitor_instance.monitor_all)
    while True:
        schedule.run_pending()
        time.sleep(60)

if __name__ == "__main__":
    # [Mode 1] ë‹¨ìˆœ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    print("ğŸ§ª [Test Mode] 1íšŒ í¬ë¡¤ë§ ë° ë¶„ì„ ì‹¤í–‰...")
    _monitor_instance.monitor_all()

    # [Mode 2] ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
    # run_continuously(interval_days=1)