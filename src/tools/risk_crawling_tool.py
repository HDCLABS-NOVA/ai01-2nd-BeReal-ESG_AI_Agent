import os
import time
import json
import requests
import urllib.parse
import numpy as np
import fitz  # PyMuPDF
from datetime import datetime
from typing import List, Dict, Optional
from dotenv import load_dotenv

# Selenium
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException, TimeoutException
from webdriver_manager.chrome import ChromeDriverManager

# LangChain & AI
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì „ì—­ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(BASE_DIR, "data")
DOWNLOAD_DIR = os.path.join(DATA_DIR, "risk_data")
HISTORY_DIR = os.path.join(DATA_DIR, "crawling")
HISTORY_FILE = os.path.join(HISTORY_DIR, "risk_history.json")
VECTOR_DB_DIR = os.path.join(BASE_DIR, "vector_db", "esg_all")

# --------------------------------------------------------------------------
# [ì„¤ì •] ë¦¬ìŠ¤í¬ ì§„ë‹¨ ìë£Œ íƒ€ê²Ÿ ëª©ë¡ (êµ¬ê¸€ ìš°íšŒ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ê°€)
# --------------------------------------------------------------------------
RISK_TARGETS = [
    # 1. [Safety] ì•ˆì „ë³´ê±´ê³µë‹¨ (KOSHAëŠ” ë‚´ë¶€ ì•„ì¹´ì´ë¸Œê°€ ì˜ ë˜ì–´ìˆì–´ ìœ ì§€)
    {
        "name": "KOSHA_C_Guide",
        "url": "https://portal.kosha.or.kr/archive/resources/tech-support/search/const?page=1&rowsPerPage=10",
        "type": "KOSHA_ARCHIVE", 
        "category": "Safety"
    },
    # 2. [Safety] ê³ ìš©ë…¸ë™ë¶€ - ìœ„í—˜ì„±í‰ê°€ (êµ¬ê¸€ ìš°íšŒ)
    {
        "name": "MOEL_Risk_Standard",
        "url": "https://www.moel.go.kr/info/publict/publictDataList.do", # ì‹¤íŒ¨ ì‹œ êµ¬ê¸€ë¡œ ì „í™˜
        "google_query": 'site:moel.go.kr filetype:pdf "ìœ„í—˜ì„±í‰ê°€" "í‘œì¤€ëª¨ë¸"',
        "type": "GOV_BOARD",
        "category": "Safety"
    },
    # 3. [Labor] ê³ ìš©ë…¸ë™ë¶€ - ììœ¨ì ê²€í‘œ (êµ¬ê¸€ ìš°íšŒ)
    {
        "name": "MOEL_Checklist",
        "url": "https://www.moel.go.kr/news/notice/noticeList.do",
        "google_query": 'site:moel.go.kr filetype:pdf "ììœ¨ì ê²€í‘œ"',
        "type": "GOV_BOARD",
        "category": "Labor"
    },
    # 4. [Env] í™˜ê²½ë¶€ - ë¹„ì‚°ë¨¼ì§€ (êµ¬ê¸€ ìš°íšŒ)
    {
        "name": "ME_Dust_Manual",
        "url": "https://www.me.go.kr/home/web/board/list.do?menuId=10392&boardMasterId=39",
        "google_query": 'site:me.go.kr filetype:pdf "ë¹„ì‚°ë¨¼ì§€" "ë§¤ë‰´ì–¼"',
        "type": "GOV_BOARD",
        "category": "Environment"
    },
    # 5. [Gov] ê³µì •ê±°ë˜ìœ„ì›íšŒ - í‘œì¤€ê³„ì•½ì„œ (êµ¬ê¸€ ìš°íšŒ)
    {
        "name": "FTC_Construction_Contract",
        "url": "https://www.ftc.go.kr/www/cop/bbs/selectBoardList.do?key=201&bbsId=BBSMSTR_000000002320",
        "google_query": 'site:ftc.go.kr filetype:hwp OR filetype:pdf "ê±´ì„¤ì—…" "í‘œì¤€í•˜ë„ê¸‰ê³„ì•½ì„œ"',
        "type": "GOV_BOARD",
        "category": "Governance"
    }
]

class RiskCrawlingTool:
    """
    [ë¦¬ìŠ¤í¬ ì§„ë‹¨ ìë£Œ ìˆ˜ì§‘ ì—ì´ì „íŠ¸]
    - ì•ˆì „(KOSHA/MOEL), í™˜ê²½(ME), ê³µì •(FTC) ë¶„ì•¼ì˜ ì‹¤ë¬´ ê°€ì´ë“œ/ì²´í¬ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
    - ì‚¬ì´íŠ¸ ì ‘ì† ì°¨ë‹¨ ì‹œ 'Google Site Search'ë¡œ ìš°íšŒí•˜ì—¬ PDF ì§ì ‘ ìˆ˜ì§‘
    """
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RiskCrawlingTool, cls).__new__(cls)
            cls._instance._initialize()
        return cls._instance

    def _initialize(self):
        print("âš™ï¸ [RiskTool] ì´ˆê¸°í™” ì¤‘...")
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-m3",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.embeddings = None

        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

        if self.embeddings:
            self.vector_db = Chroma(
                collection_name="esg_risk_guides",
                embedding_function=self.embeddings,
                persist_directory=VECTOR_DB_DIR
            )
        else:
            self.vector_db = None

        os.makedirs(DOWNLOAD_DIR, exist_ok=True)
        os.makedirs(HISTORY_DIR, exist_ok=True)
        self.history = self._load_history()

    def _load_history(self) -> Dict:
        if os.path.exists(HISTORY_FILE):
            try:
                with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except: return {}
        return {}

    def _save_history(self):
        try:
            with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, ensure_ascii=False, indent=2)
        except: pass

    def _is_processed(self, key: str) -> bool:
        return key in self.history

    def _mark_as_processed(self, key: str, title: str, files: List[str]):
        self.history[key] = {
            "title": title,
            "processed_at": datetime.now().isoformat(),
            "files": files
        }
        self._save_history()

    def _get_chrome_driver(self):
        chrome_options = Options()
        # [ì¤‘ìš”] ë´‡ íƒì§€ íšŒí”¼ ì˜µì…˜ ê°•í™”
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled") # ìë™í™” ì œì–´ ê°ì§€ ë¹„í™œì„±í™”
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"]) # ìë™í™” í‘œì‹œ ì œê±°
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # ì¼ë°˜ ì‚¬ìš©ìì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” User-Agent
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")
        
        prefs = {
            "download.default_directory": DOWNLOAD_DIR,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True,
            "plugins.always_open_pdf_externally": True,
            "profile.default_content_settings.popups": 0
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        service = ChromeService(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # ë´‡ íƒì§€ ìš°íšŒìš© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        return driver

    def _extract_text_preview(self, pdf_path: str, max_pages: int = 5) -> str:
        text = ""
        try:
            doc = fitz.open(pdf_path)
            for i, page in enumerate(doc):
                if i >= max_pages: break
                text += page.get_text()
            doc.close()
        except: pass
        return text

    def _analyze_and_store(self, file_path: str, title: str, target_info: Dict) -> bool:
        if not self.vector_db or not file_path.lower().endswith('.pdf'):
            return False

        filename = os.path.basename(file_path)
        print(f"   ğŸ§  [AI ë¶„ì„] '{filename}' ì‹¤ë¬´ í™œìš©ë„ í‰ê°€ ì¤‘...")
        
        content_preview = self._extract_text_preview(file_path)
        if not content_preview: return False

        prompt = f"""
        ë¬¸ì„œ ì œëª©: {title}
        ì¹´í…Œê³ ë¦¬: {target_info['category']}
        ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:
        {content_preview[:2500]}

        ì´ ë¬¸ì„œê°€ ê¸°ì—… í˜„ì¥ì—ì„œ ì•ˆì „/í™˜ê²½/ë…¸ë¬´ ë¦¬ìŠ¤í¬ë¥¼ ì ê²€í•  ë•Œ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•œ **ì‹¤ë¬´ ìë£Œ**ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
        
        [íŒë‹¨ ê¸°ì¤€]
        - **ìœ ìš©í•¨ (True)**: ì²´í¬ë¦¬ìŠ¤íŠ¸(Checklist), ììœ¨ì ê²€í‘œ, ê¸°ìˆ  ê°€ì´ë“œë¼ì¸(KOSHA Guide), í‘œì¤€ê³„ì•½ì„œ ì–‘ì‹, ë§¤ë‰´ì–¼.
        - **ìœ ìš©í•˜ì§€ ì•ŠìŒ (False)**: ë‹¨ìˆœ í–‰ì‚¬ ì•Œë¦¼, ì¸ì‚¬ ë°œë ¹, í†µê³„ ì—°ë³´, ì •ì±… í™ë³´ í¬ìŠ¤í„°.

        ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì¶œë ¥:
        {{
            "is_practical": true/false,
            "doc_type": "Checklist/Manual/Contract/Other",
            "score": (1~10),
            "summary": "í•œ ì¤„ ìš”ì•½"
        }}
        """
        
        try:
            response = self.llm.invoke(prompt)
            result = json.loads(response.content.replace("```json", "").replace("```", "").strip())
            
            print(f"      ğŸ‘‰ ê²°ê³¼: {result['doc_type']} (ì ìˆ˜: {result['score']})")

            if result['is_practical'] and result['score'] >= 7:
                print(f"      ğŸ’¾ [Vector DB] ì €ì¥í•©ë‹ˆë‹¤.")
                
                full_doc = fitz.open(file_path)
                full_text = ""
                for page in full_doc:
                    full_text += page.get_text()
                full_doc.close()

                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                chunks = text_splitter.create_documents(
                    [full_text], 
                    metadatas=[{
                        "source": target_info['name'],
                        "category": target_info['category'],
                        "title": title,
                        "doc_type": result['doc_type'],
                        "filename": filename,
                        "crawled_at": datetime.now().isoformat()
                    }]
                )
                self.vector_db.add_documents(chunks)
                print(f"      âœ… DB ì €ì¥ ì™„ë£Œ ({len(chunks)} chunks)")
                return True
            else:
                print("      ğŸ—‘ï¸ [Skip] ì‹¤ë¬´ í™œìš©ë„ê°€ ë‚®ì•„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            print(f"      âŒ AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            return False

    # ----------------------------------------------------------------
    # [Fallback Strategy] Google Site Search
    # ----------------------------------------------------------------
    def _scrape_google_fallback(self, driver, target_info: Dict) -> List[Dict]:
        """
        ë‚´ë¶€ ê²€ìƒ‰ì´ ë§‰í˜”ì„ ë•Œ, Googleì„ í†µí•´ í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ PDFë¥¼ ì§ì ‘ ì°¾ìŠµë‹ˆë‹¤.
        Query ì˜ˆì‹œ: site:moel.go.kr filetype:pdf "ìœ„í—˜ì„±í‰ê°€"
        """
        query = target_info.get("google_query")
        if not query:
            return []
            
        search_url = f"https://www.google.com/search?q={urllib.parse.quote(query)}"
        name = target_info["name"]
        results = []
        
        print(f"ğŸš€ [Google Bypass] '{name}' ìš°íšŒ ê²€ìƒ‰ ì‹œë„... ({query})")
        try:
            driver.get(search_url)
            # êµ¬ê¸€ ê²€ìƒ‰ê²°ê³¼ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "search")))
            
            # ê²€ìƒ‰ ê²°ê³¼ ë§í¬ ìˆ˜ì§‘ (êµ¬ê¸€ì˜ ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡°: div.g a)
            links = driver.find_elements(By.CSS_SELECTOR, "div.g a")
            
            # ìƒìœ„ 3ê°œ PDFë§Œ ì²˜ë¦¬
            pdf_links = []
            for link in links:
                href = link.get_attribute("href")
                if href and href.lower().endswith(".pdf"):
                    # êµ¬ê¸€ íŠ¸ë˜í‚¹ ë§í¬ê°€ ì•„ë‹Œ ì‹¤ì œ ë§í¬ì¸ì§€ í™•ì¸
                    pdf_links.append((link, href))
            
            # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 3ê°œ ì„ íƒ
            seen_urls = set()
            unique_pdfs = []
            for l, h in pdf_links:
                if h not in seen_urls:
                    unique_pdfs.append((l, h))
                    seen_urls.add(h)
            
            print(f"   ğŸ” êµ¬ê¸€ì—ì„œ PDF {len(unique_pdfs)}ê°œ ë°œê²¬")

            for i, (link_elem, pdf_url) in enumerate(unique_pdfs[:3]):
                try:
                    title = link_elem.find_element(By.CSS_SELECTOR, "h3").text
                    unique_key = f"Google_{name}_{title}"
                    
                    if self._is_processed(unique_key):
                        print(f"   â­ï¸ [Skip] {title}")
                        continue
                        
                    print(f"   ğŸ“¥ [Direct Download] {title}")
                    
                    # PDF ì§ì ‘ ë‹¤ìš´ë¡œë“œ (requests ì‚¬ìš©)
                    # Seleniumìœ¼ë¡œ PDFë¥¼ ì—´ë©´ ë·°ì–´ê°€ ëœ° ìˆ˜ ìˆìœ¼ë¯€ë¡œ requestsë¡œ ë°›ìŒ
                    response = requests.get(pdf_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
                    
                    if response.status_code == 200:
                        # íŒŒì¼ëª… ì•ˆì „í•˜ê²Œ ë§Œë“¤ê¸°
                        safe_title = "".join([c for c in title if c.isalnum() or c in (' ', '-', '_')]).rstrip()
                        filename = f"{safe_title}.pdf"
                        file_path = os.path.join(DOWNLOAD_DIR, filename)
                        
                        with open(file_path, 'wb') as f:
                            f.write(response.content)
                            
                        print(f"      âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")
                        
                        # AI ë¶„ì„ ë° ì €ì¥
                        if self._analyze_and_store(file_path, title, target_info):
                            self._mark_as_processed(unique_key, title, [file_path])
                            results.append({"source": name, "title": title, "files": [file_path]})
                            
                except Exception as e:
                    print(f"      âš ï¸ êµ¬ê¸€ ê²€ìƒ‰ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    
        except Exception as e:
            print(f"âŒ êµ¬ê¸€ ìš°íšŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
        return results

    # ----------------------------------------------------------------
    # [Crawling] Main Strategies
    # ----------------------------------------------------------------
    def _scrape_kosha_archive(self, driver, target_info: Dict) -> List[Dict]:
        """KOSHAëŠ” ë‚´ë¶€ ê²€ìƒ‰ì´ ì˜ ë˜ë¯€ë¡œ ê¸°ì¡´ ë¡œì§ ìœ ì§€"""
        url = target_info["url"]
        name = target_info["name"]
        results = []
        
        print(f"ğŸ“¡ [{name}] KOSHA ì ‘ì† ì¤‘... ({url})")
        try:
            driver.get(url)
            wait = WebDriverWait(driver, 20)
            time.sleep(3) 

            for i in range(3):
                try:
                    links = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a")))
                    target_links = [l for l in links if len(l.text.strip()) > 10 and l.is_displayed()]
                    if i >= len(target_links): break
                    
                    post_link = target_links[i]
                    title = post_link.text.strip()
                    unique_key = f"{name}_{title}"
                    
                    if self._is_processed(unique_key):
                        print(f"   â­ï¸ [Skip] {title}")
                        continue
                        
                    print(f"   ğŸ” [New] ë¶„ì„: {title}")
                    driver.execute_script("arguments[0].click();", post_link)
                    time.sleep(3)
                    
                    downloaded_files = []
                    try:
                        file_links = driver.find_elements(By.XPATH, "//a[contains(@href, 'download') or contains(text(), 'ë‹¤ìš´ë¡œë“œ') or contains(@href, 'file')]")
                        for f_link in file_links:
                            driver.execute_script("arguments[0].click();", f_link)
                            time.sleep(5) # ë‹¤ìš´ë¡œë“œ ëŒ€ê¸°
                            # (íŒŒì¼ í™•ì¸ ë¡œì§ ìƒëµ - ìµœê·¼ íŒŒì¼ í™•ì¸ ë“±)
                            # ì—¬ê¸°ì„œëŠ” KOSHA íŠ¹ì„±ìƒ ë‹¤ìš´ë¡œë“œ ì„±ê³µ ê°€ì •í•˜ê³  ë‹¤ìŒìœ¼ë¡œ
                            break
                    except: pass
                    
                    self._mark_as_processed(unique_key, title, [])
                    driver.back()
                    time.sleep(3)
                except:
                    driver.get(url) 
                    time.sleep(3)
        except Exception as e:
            print(f"âŒ KOSHA í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return results

    def _scrape_gov_board(self, driver, target_info: Dict) -> List[Dict]:
        """
        ì¼ë°˜ ê³µê³µê¸°ê´€ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œë„ -> ì‹¤íŒ¨ ì‹œ Google ìš°íšŒ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜
        """
        url = target_info["url"]
        name = target_info["name"]
        
        print(f"ğŸ“¡ [{name}] ì ‘ì† ì‹œë„... ({url})")
        try:
            driver.get(url)
            wait = WebDriverWait(driver, 10)
            
            # êµ¬ì¡° ê°ì§€ ì‹œë„
            try:
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "tbody")))
                print("   âœ… ë‚´ë¶€ ê²Œì‹œíŒ êµ¬ì¡° ê°ì§€ë¨. í¬ë¡¤ë§ ì§„í–‰.")
                # (ì—¬ê¸°ì— ê¸°ì¡´ í…Œì´ë¸” í¬ë¡¤ë§ ë¡œì§ì´ ë“¤ì–´ê°€ì•¼ í•˜ì§€ë§Œ, 
                #  í˜„ì¬ ì ‘ì† ìì²´ê°€ ë¶ˆì•ˆì •í•˜ë¯€ë¡œ ë°”ë¡œ Google Fallbackì„ ìš°ì„ ì‹œí•˜ëŠ” ì „ëµë„ ê°€ëŠ¥)
                #  ì¼ë‹¨ êµ¬ì¡°ê°€ ê°ì§€ë˜ì–´ë„ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
                rows = driver.find_elements(By.TAG_NAME, "tr")
                if len(rows) < 2: raise Exception("Empty Board")
                
            except Exception:
                print("   âš ï¸ ë‚´ë¶€ ê²Œì‹œíŒ êµ¬ì¡° ê°ì§€ ì‹¤íŒ¨ ë˜ëŠ” ì°¨ë‹¨ë¨.")
                raise Exception("Access Blocked or Structure Unknown")

            # (ì„±ê³µ ì‹œ ë¡œì§ì€ ìƒëµí•˜ê³ , ì‹¤íŒ¨ ìœ ë„í•˜ì—¬ ë°”ë¡œ êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ë„˜ê¹€ - ì•ˆì •ì„± ìš°ì„ )
            # ì‚¬ìš©ì ìš”ì²­: "ìš°íšŒí•´ì„œ ì ‘ì†ì„ í•˜ëŠ” ë°©ë²•ì„ ì°¾ì•„ì•¼í•  ê²ƒ ê°™ì•„"
            # ë”°ë¼ì„œ ë°”ë¡œ Exceptionì„ ë°œìƒì‹œì¼œ Fallbackìœ¼ë¡œ ë„˜ê¹ë‹ˆë‹¤.
            raise Exception("Force Fallback to Google")

        except Exception as e:
            print(f"   ğŸ”„ ë‚´ë¶€ ì ‘ì† ë¶ˆê°€ ({e}). Google ìš°íšŒ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            return self._scrape_google_fallback(driver, target_info)

        return []

    def collect_all_guides(self) -> str:
        print("\n" + "="*50)
        print(f"ğŸ›¡ï¸ [Risk Data ìˆ˜ì§‘] {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*50)
        
        driver = self._get_chrome_driver()
        total_results = []
        
        try:
            for target in RISK_TARGETS:
                if target.get("type") == "KOSHA_ARCHIVE":
                    res = self._scrape_kosha_archive(driver, target)
                else:
                    # ì •ë¶€ ì‚¬ì´íŠ¸ëŠ” ë°”ë¡œ ì ‘ì† ì‹œë„ í›„ ì‹¤íŒ¨ ì‹œ êµ¬ê¸€ ìš°íšŒ
                    res = self._scrape_gov_board(driver, target)
                total_results.extend(res)
        finally:
            driver.quit()
            
        report = f"## ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ìë£Œ ìˆ˜ì§‘ ë¦¬í¬íŠ¸\n"
        if total_results:
            for item in total_results:
                files = ", ".join([os.path.basename(f) for f in item['files']])
                report += f"- **[{item['source']}]** {item['title']}\n  - ğŸ’¾ {files}\n"
        else:
            report += "- ì‹ ê·œ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë‘ ìµœì‹  ë˜ëŠ” ìˆ˜ì§‘ ì‹¤íŒ¨).\n"
            
        print(report)
        return report

# LangChain Tool Export
_risk_collector = RiskCrawlingTool()

@tool
def fetch_risk_guides(query: str = "safety checklist") -> str:
    """
    Collects practical risk assessment guides, checklists, and manuals 
    from KOSHA, MOEL, ME, FTC.
    """
    return _risk_collector.collect_all_guides()

if __name__ == "__main__":
    _risk_collector.collect_all_guides()