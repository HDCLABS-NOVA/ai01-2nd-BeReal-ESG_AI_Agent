from pathlib import Path
import shutil
import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import io
import re
from collections import Counter
from typing import Iterable
import numpy as np
import hashlib

try:
    from openparse.doc_parser import DocumentParser
except ImportError:  # pragma: no cover - optional dependency
    DocumentParser = None

try:
    from paddleocr import PaddleOCR
except ImportError:  # pragma: no cover - optional dependency
    PaddleOCR = None

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langdetect import detect


# 0. ê¸°ë³¸ ì„¤ì •
DATA_DIR = Path("data")
VECTOR_DIR = "vector_db/esg_all"

# HuggingFace ì„ë² ë”© (4060 GPU í™œìš© ê°€ëŠ¥)
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",      # ë‹¤êµ­ì–´ ì§€ì›, ì„±ëŠ¥/ì†ë„ ê´œì°®ìŒ
    # encode_kwargs={"normalize_embeddings": True},  # ì„ íƒ ì˜µì…˜
)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200,
    chunk_overlap=150,
    separators=["\n\n", "\n", ".", " "]
)

LAYOUT_KEYWORDS = {
    "CONTENTS",
    "TABLE OF CONTENTS",
    "INDEX",
    "SUSTAINABILITY REPORT",
}

NAV_MENU_WORDS = {
    "OVERVIEW",
    "ENVIRONMENTAL",
    "SOCIAL",
    "GOVERNANCE",
    "APPENDIX",
}

COUNTRY_BY_SOURCE = {
    "domestic": "KR",
    "companies": "KR",
    "global": "GLOBAL",
}

OPENPARSE_TARGET_FILES: set[str] | None = None  # ì „ì²´ companies ë¬¸ì„œì— OpenParse ì ìš©


def infer_pdf_metadata(pdf_path: Path, source_type: str) -> dict:
    """Extract company/year/country metadata from filename and folder."""

    stem = pdf_path.stem
    company = stem.split("_")[0].strip()
    year_match = re.search(r"(20\\d{2})", stem)
    year = year_match.group(1) if year_match else None

    meta = {}
    if company:
        meta["company"] = company
    if year:
        meta["year"] = year

    country = COUNTRY_BY_SOURCE.get(source_type)
    if country:
        meta["country"] = country
    return meta


# -------------------------------------------------------
# 1. í…ìŠ¤íŠ¸/OCR ì¶”ì¶œ ë„ìš°ë¯¸
# -------------------------------------------------------
def _load_pdf_pages_pymupdf(pdf_path, source_type):
    doc = fitz.open(pdf_path)
    pages = []
    for idx, page in enumerate(doc):
        text = page.get_text("text") or ""
        pages.append(
            Document(
                page_content=text,
                metadata={
                    "source_file": Path(pdf_path).name,
                    "source_type": source_type,
                    "page": idx + 1,
                },
            )
    )
    return pages


_OPENPARSE_PARSER = None
OPENPARSE_PREVIEW_NODES = 2  # OpenParse ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê¸° ìœ„í•œ ìƒ˜í”Œ ê°œìˆ˜


def should_use_openparse(pdf_path: Path, source_type: str) -> bool:
    return DocumentParser is not None and source_type == "companies"


def get_openparse_parser() -> DocumentParser:
    """OpenParse ë¬¸ì„œ íŒŒì„œë¥¼ 1íšŒë§Œ ìƒì„±."""

    if DocumentParser is None:
        raise RuntimeError("openparse íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    global _OPENPARSE_PARSER
    if _OPENPARSE_PARSER is None:
        table_args = {
            "parsing_algorithm": "table-transformers",
            "table_output_format": "markdown",
            "min_table_confidence": 0.4,
            "min_cell_confidence": 0.2,
        }
        _OPENPARSE_PARSER = DocumentParser(table_args=table_args)
    return _OPENPARSE_PARSER


def _node_to_text(node) -> str:
    parts = []
    for element in getattr(node, "elements", []):
        text = getattr(element, "text", "")
        if text:
            parts.append(text.strip())
    return "\n".join(part for part in parts if part).strip()


def _node_page(node) -> int | None:
    bboxes = getattr(node, "bbox", [])
    pages = [bbox.page for bbox in bboxes if hasattr(bbox, "page")]
    return min(pages) if pages else None


def _load_pdf_pages_openparse(pdf_path: Path, source_type: str):
    parser = get_openparse_parser()
    parsed = parser.parse(str(pdf_path), ocr=False)
    documents = []
    for idx, node in enumerate(parsed.nodes, start=1):
        text = _node_to_text(node)
        if not text:
            continue
        page_no = _node_page(node) or idx
        metadata = {
            "source_file": pdf_path.name,
            "source_type": source_type,
            "page": page_no,
            "parser": "openparse",
        }
        if idx <= OPENPARSE_PREVIEW_NODES:
            print(
                f"\n[OpenParse] {pdf_path.name} node {idx} preview:\n"
                f"{text[:500]}\n{'-' * 60}"
            )
        documents.append(
            Document(
                page_content=text,
                metadata=metadata,
            )
        )
    return documents


def load_pdf_pages(pdf_path, source_type):
    """íŠ¹ì • companies ë¬¸ì„œëŠ” OpenParse, ë‚˜ë¨¸ì§€ëŠ” PyMuPDF ì‚¬ìš©."""

    pdf_path = Path(pdf_path)
    if should_use_openparse(pdf_path, source_type):
        try:
            return _load_pdf_pages_openparse(pdf_path, source_type)
        except Exception as exc:
            print(f"[OpenParse] ì‹¤íŒ¨, PyMuPDFë¡œ ëŒ€ì²´ ({pdf_path.name}): {exc}")
    return _load_pdf_pages_pymupdf(str(pdf_path), source_type)


def extract_images_from_pdf(pdf_path, target_pages=None):
    doc = fitz.open(pdf_path)
    texts = []
    targets = set(target_pages or [])

    for page_index, page in enumerate(doc):
        page_number = page_index + 1
        if targets and page_number not in targets:
            continue

        images = page.get_images()
        for img_index, img in enumerate(images):
            xref = img[0]
            base = doc.extract_image(xref)
            image_bytes = base["image"]
            pil_img = Image.open(io.BytesIO(image_bytes))

            ocr_text = perform_ocr(pil_img)
            ocr_text = normalize_ocr_text(ocr_text)
            if len(ocr_text.strip()) > 10:
                texts.append((page_number, ocr_text))

    return texts  # [(page, text), ...]


# -------------------------------------------------------
# 2. ìë™ í—¤ë”/í‘¸í„° íƒì§€
# -------------------------------------------------------
def looks_like_navigation_ui(text: str) -> bool:
    upper = text.upper()
    nav_hits = sum(1 for word in NAV_MENU_WORDS if word in upper)
    if nav_hits >= 4:
        return True
    return any(keyword in upper for keyword in LAYOUT_KEYWORDS)


def is_navigation_line(line: str) -> bool:
    """í—¤ë”/ëª©ì°¨ ì „ìš© ë¼ì¸ì„ íƒì§€í•˜ì—¬ í•„í„°ë§í•œë‹¤."""

    stripped = line.strip()
    if not stripped:
        return False

    upper = stripped.upper()
    if upper in NAV_MENU_WORDS or upper in LAYOUT_KEYWORDS:
        return True

    tokens = [tok for tok in re.split(r"[\sÂ·|]+", upper) if tok]
    if not tokens:
        return False

    nav_hits = sum(1 for tok in tokens if tok in NAV_MENU_WORDS)
    if nav_hits >= 3 and nav_hits == len(tokens):
        return True
    if nav_hits >= 4:
        return True
    return False


def is_valid_header_footer_line(line: str) -> bool:
    stripped = line.strip()
    if not stripped:
        return False
    if len(stripped) > 60:
        return False
    token_count = len(re.findall(r"[A-Za-zê°€-í£]+", stripped))
    if token_count >= 10:
        return False
    if looks_like_navigation_ui(stripped):
        return False
    return True


def detect_repeating_headers_footers(page_texts, top_n=3, bottom_n=3):
    header_counter = Counter()
    footer_counter = Counter()

    def filtered_lines(lines):
        return [line.strip() for line in lines if is_valid_header_footer_line(line)]

    for txt in page_texts:
        lines = txt.split("\n")
        header_counter.update(filtered_lines(lines[:top_n]))
        footer_counter.update(filtered_lines(lines[-bottom_n:]))

    total_pages = len(page_texts)
    threshold = max(2, int(total_pages * 0.6))
    common_headers = {h for h, c in header_counter.items() if c >= threshold}
    common_footers = {f for f, c in footer_counter.items() if c >= threshold}

    return common_headers, common_footers


# -------------------------------------------------------
# 3. ë³¸ë¬¸ ì •ì œ í•¨ìˆ˜
# -------------------------------------------------------
def drop_garbage_lines(text: str) -> str:
    cleaned_lines = []
    for line in text.split("\n"):
        stripped = line.strip()
        if not stripped:
            continue
        if is_navigation_line(stripped):
            continue
        if stripped.isdigit():
            continue
        if re.fullmatch(r"[IVXLCDM]+", stripped):
            continue
        if re.fullmatch(r"[A-Z]{1,3}", stripped):
            continue
        words = re.findall(r"[A-Za-zê°€-í£]+", stripped)
        if words and all(len(w) <= 2 for w in words):
            continue
        cleaned_lines.append(stripped)
    return "\n".join(cleaned_lines)


def clean_text_basic(text):
    if not text:
        return None

    filtered = drop_garbage_lines(text)
    if not filtered:
        return None

    t = filtered.strip()

    # ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œê±°
    if len(t) < 10:
        return None

    # ìˆ«ì/ê¸°í˜¸ ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ì€ ë¼ì¸ì€ garbage
    if sum(c.isalpha() for c in t) / (len(t) + 1) < 0.2:
        return None

    return t


def strip_header_footer(text, headers, footers):
    lines = text.split("\n")
    cleaned = [
        line for line in lines
        if line.strip() not in headers and line.strip() not in footers
    ]
    return "\n".join(cleaned).strip()


def should_skip_page(text: str, page_number: int) -> (bool, str | None):
    if page_number == 1:
        return True, "cover"

    upper = text.upper()

    if page_number <= 3:
        if looks_like_navigation_ui(text):
            return True, "nav_ui"
        if any(keyword in upper for keyword in LAYOUT_KEYWORDS):
            return True, "layout_keyword"

    return False, None


def page_needs_ocr(text: str) -> bool:
    if not text or not text.strip():
        return True
    alpha_chars = sum(c.isalpha() for c in text)
    return alpha_chars < 15


# -------------------------------------------------------
# 4. ë‹¨ì¼ PDF â†’ Documents
# -------------------------------------------------------
def perform_ocr(image: Image.Image) -> str:
    """PaddleOCRê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ í•œê¸€+ì˜ë¬¸ ëª¨ë¸ë¡œ ì¸ì‹í•˜ê³ , ì—†ìœ¼ë©´ Tesseract ì‚¬ìš©."""

    if PaddleOCR is not None:
        if "_PADDLE_OCR" not in globals():
            # í•œêµ­ì–´/ì˜ë¬¸ ëª¨ë‘ í¬í•¨í•˜ëŠ” ë‹¤êµ­ì–´ ëª¨ë¸ ì´ˆê¸°í™” (ë¹„ë™ê¸° ë¡œë”© ë°©ì§€)
            globals()["_PADDLE_OCR"] = PaddleOCR(
                lang="korean",
                use_angle_cls=True,
                show_log=False,
            )
        ocr_engine = globals()["_PADDLE_OCR"]
        np_img = np.array(image.convert("RGB"))
        result = ocr_engine.ocr(np_img, cls=True)
        texts = []
        if result and result[0]:
            for line in result[0]:
                value = line[1][0]
                if value:
                    texts.append(value)
        if texts:
            return "\n".join(texts)

    # ë°±ì—…: ê¸°ì¡´ Tesseract ì „ëµ ìœ ì§€
    return pytesseract.image_to_string(image, lang="kor+eng")


def normalize_ocr_text(text: str) -> str:
    """ê°„ë‹¨í•œ ì–¸ì–´ ê°ì§€ í›„ í•œê¸€/ì˜ë¬¸ ì „ìš© ì •ê·œí™”ë¥¼ ì ìš©í•œë‹¤."""

    stripped = text.strip()
    if not stripped:
        return stripped

    try:
        lang = detect(stripped)
    except Exception:
        lang = "ko"

    if lang.startswith("ko"):
        return normalize_korean_text(stripped)
    return normalize_english_text(stripped)


def normalize_korean_text(text: str) -> str:
    # ìëª¨ ë¶„ë¦¬/ê²°í•© ë¬¸ì œë¥¼ ê°„ë‹¨íˆ ì •ë¦¬í•˜ê³ , íŠ¹ìˆ˜ë¬¸ì ë…¸ì´ì¦ˆë¥¼ ì œê±°í•œë‹¤.
    cleaned = re.sub(r"[^0-9A-Za-zê°€-í£.,;:()\-\s]", " ", text)
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def normalize_english_text(text: str) -> str:
    # ì˜ì–´ OCR ê²°ê³¼ëŠ” ASCII ë¬¸ì ìœ„ì£¼ë¡œ ì •ë¦¬í•˜ê³  ë‹¤ì¤‘ ê³µë°±ì„ ì¶•ì†Œí•œë‹¤.
    cleaned = re.sub(r"[^0-9A-Za-z.,;:()'\-\s]", " ", text)
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def process_pdf(pdf_path, source_type):
    print(f"\n========== Processing: {pdf_path.name} ==========\n")

    pages = load_pdf_pages(str(pdf_path), source_type)
    page_texts = [p.page_content for p in pages]

    headers, footers = detect_repeating_headers_footers(page_texts)

    base_meta = infer_pdf_metadata(pdf_path, source_type)

    cleaned_pages = []
    ocr_targets = []
    qc_events = []
    for i, p in enumerate(pages):
        raw_text = p.page_content
        page_num = i + 1

        skip, reason = should_skip_page(raw_text, page_num)
        if skip:
            qc_events.append((page_num, "skip", reason))
            continue

        # í—¤ë”/í‘¸í„° ì œê±°
        cleaned = strip_header_footer(raw_text, headers, footers)
        cleaned = clean_text_basic(cleaned)
        if not cleaned:
            if page_needs_ocr(raw_text):
                ocr_targets.append(page_num)
                qc_events.append((page_num, "ocr_candidate", "low_text"))
            else:
                qc_events.append((page_num, "drop", "clean_failed"))
            continue

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        p.page_content = cleaned
        metadata = p.metadata or {}
        metadata.update({
            "source_file": pdf_path.name,
            "source_type": source_type,
            "page": page_num,
        })
        metadata.update(base_meta)
        p.metadata = metadata
        cleaned_pages.append(p)

    # ì´ë¯¸ì§€ OCR ì¶”ê°€ (í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•œ í˜ì´ì§€ë§Œ)
    ocr_list = extract_images_from_pdf(str(pdf_path), target_pages=ocr_targets)
    ocr_docs = []
    for page_num, text in ocr_list:
        cleaned = clean_text_basic(text)
        if cleaned:
            ocr_meta = {
                "source_file": pdf_path.name,
                "source_type": source_type,
                "ocr": True,
                "page": page_num,
            }
            ocr_meta.update(base_meta)
            ocr_docs.append(
                Document(
                    page_content=cleaned,
                    metadata=ocr_meta,
                )
            )

    # chunking
    chunks = text_splitter.split_documents(cleaned_pages)
    # ê°™ì€ í˜ì´ì§€ì—ì„œ ë‚˜ì˜¨ ì¤‘ë³µ ì²­í¬ëŠ” build ë‹¨ê³„ ì „ì— ì •ë¦¬í•œë‹¤.
    chunks = deduplicate_chunks(chunks)

    # OCR chunk (ìˆ˜ë™ êµ¬ì¡°)
    for od in ocr_docs:
        assign_chunk_id(od)
        chunks.append(od)

    return chunks, headers, footers, qc_events


# -------------------------------------------------------
# 5. ì „ì²´ PDF ingest
# -------------------------------------------------------
def assign_chunk_id(doc: Document) -> str:
    payload = "|".join(
        [
            str(doc.metadata.get("source_file")),
            str(doc.metadata.get("page")),
            doc.page_content.strip(),
        ]
    )
    chunk_id = hashlib.sha1(payload.encode("utf-8")).hexdigest()
    doc.metadata["chunk_id"] = chunk_id
    return chunk_id


def deduplicate_chunks(docs: Iterable[Document]):
    """ê°™ì€ chunk_idê°€ ë°˜ë³µë˜ë©´ ì²« ì²­í¬ë§Œ ìœ ì§€."""

    unique = []
    seen_ids = set()
    for doc in docs:
        text = doc.page_content.strip()
        if not text:
            continue
        chunk_id = assign_chunk_id(doc)
        if chunk_id in seen_ids:
            continue
        seen_ids.add(chunk_id)
        unique.append(doc)
    return unique


def load_existing_chunk_ids(persist_dir: Path):
    if not persist_dir.exists():
        return set(), None
    vectordb = Chroma(
        persist_directory=str(persist_dir),
        collection_name="esg_all",
        embedding_function=embedding_model,
    )
    existing = vectordb.get(include=["metadatas"])
    chunk_ids = set()
    for meta in existing.get("metadatas", []) or []:
        chunk_id = meta.get("chunk_id") if meta else None
        if chunk_id:
            chunk_ids.add(chunk_id)
    return chunk_ids, vectordb


def build_vector_db(clear_existing: bool = False):
    persist_dir = Path(VECTOR_DIR)
    if clear_existing and persist_dir.exists():
        print(f"[VectorDB] ê¸°ì¡´ ì €ì¥ì†Œ ì‚­ì œ â†’ {persist_dir}")
        shutil.rmtree(persist_dir)

    existing_ids, vectordb = load_existing_chunk_ids(persist_dir)
    new_chunks = []

    for folder in ["domestic", "global", "companies"]:
        path = DATA_DIR / folder
        if not path.exists():
            continue

        for pdf_file in path.glob("*.pdf"):
            chunks, headers, footers, qc_events = process_pdf(pdf_file, folder)
            for doc in chunks:
                chunk_id = doc.metadata.get("chunk_id") or assign_chunk_id(doc)
                if chunk_id in existing_ids:
                    continue
                existing_ids.add(chunk_id)
                new_chunks.append(doc)

            # ---- ìƒ˜í”Œ QC ì¶œë ¥ ----
            # print("\n[QC] í—¤ë” íŒ¨í„´ íƒì§€ ê²°ê³¼:")
            # print(headers)
            # print("[QC] í‘¸í„° íŒ¨í„´ íƒì§€ ê²°ê³¼:")
            # print(footers)

            # print("[QC] í˜ì´ì§€ ì²˜ë¦¬ ê²°ê³¼ (ì• 5ê°œ):")
            # for event in qc_events[:5]:
            #     page_no, status, reason = event
            #     print(f"  - page {page_no}: {status} ({reason})")

            # print(f"\n[QC] ìƒ˜í”Œ Chunk ì¶œë ¥ (ì• 2ê°œ):")
            # for c in chunks[:2]:
            #     print("\n----- CHUNK SAMPLE -----")
            #     print(c.page_content[:400])
            #     print(c.metadata)

    if not new_chunks:
        print("\nâš ï¸  ì¶”ê°€í•  ì‹ ê·œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ VectorDBë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n")
        return

    if vectordb is None:
        vectordb = Chroma.from_documents(
            documents=new_chunks,
            embedding=embedding_model,
            persist_directory=VECTOR_DIR,
            collection_name="esg_all",
        )
    else:
        vectordb.add_documents(new_chunks)

    vectordb.persist()
    print(f"\nğŸš€ VectorDB ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì‹ ê·œ ì²­í¬ {len(new_chunks)}ê°œ) â†’ {VECTOR_DIR}\n")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ESG VectorDB builder")
    parser.add_argument(
        "--clear",
        action="store_true",
        help="ê¸°ì¡´ vector_db/esg_all ë””ë ‰í„°ë¦¬ë¥¼ ì‚­ì œí•œ ë’¤ ì „ì²´ ì¬êµ¬ì¶•",
    )
    args = parser.parse_args()

    build_vector_db(clear_existing=args.clear)
